# Kubernetes-Argocd Project (App of Apps)
Project Overview
GitOps-Driven Microservices Deployment on Kubernetes (EKS / Minikube)
A production-style Kubernetes microservices project using GitOps with Argo CD.
Demonstrates secure service-to-service communication, ingress management, and MongoDB integration.
Focused on real-world debugging, Kustomize overlays, and operational best practices.

Steps
1. Deploy minikube or Docker Desktop Cluster
a. Run choco install minikube or 
b. Go to kubernetes on docker desktop and select kind cluster type

2.  Install argocd 
kubectl create namespace argocd
kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml
kubectl get pods -n argocd

# Access Argocd GUI
kubectl port-forward svc/argocd-server -n argocd 8080:80

# Argocd password
kubectl get secret argocd-initial-admin-secret -n argocd -o jsonpath="{.data.password}" | base64 --decode

3. Install nginx alb controller
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/cloud/deploy.yaml

4. Prepare your manifest files using the structure below
repo-root/
â”œâ”€â”€ k8s/
â”‚   â”œâ”€â”€ bootstrap/
â”‚   â”‚   â””â”€â”€ root-app-dev.yaml      # App of Apps 
â”‚   â””â”€â”€ apps/
â”‚       â””â”€â”€ dev/
â”‚           â”œâ”€â”€ web-app/
â”‚           â”‚   â”œâ”€â”€ deployment.yaml
â”‚           â”‚   â”œâ”€â”€ service.yaml
â”‚           â”‚   â”œâ”€â”€ ingress.yaml
â”‚           â”‚   
â”‚           â”œâ”€â”€ token-app/
â”‚           â”‚   â”œâ”€â”€ deployment.yaml
â”‚           â”‚   â”œâ”€â”€ service.yaml
â”‚           â”‚   â”œâ”€â”€ ingress.yaml
â”‚           â”‚   
â”‚           â””â”€â”€ payment-app/
â”‚               â”œâ”€â”€ mongo-deployment.yaml
â”‚               â”œâ”€â”€ mongo-service.yaml
â”‚               â”œâ”€â”€ web-deployment.yaml
â”‚               â”œâ”€â”€ web-service.yaml
â”‚               â”œâ”€â”€ configmap.yaml
â”‚               â”œâ”€â”€ secret.yaml
â”‚               â””â”€â”€ web-ingress.yaml

| Component         | Exposure       |
| ----------------- | -------------- |
| Web App           | Ingress        |
| Token App         | Ingress        |
| Payment (MongoDB) | ClusterIP only |
| Debugging         | Port-forward   |

# to delete applications that are stuck
Remove the finalizer (this is the key step)
Run:
kubectl patch application payment-app -n argocd --type=json -p='[{"op":"remove","path":"/metadata/finalizers"}]'
kubectl get applications -n argocd
kubectl delete application payment-app -n argocd

kubectl rollout restart deployment argocd-server -n argocd
Nuclear option (rarely needed)
Only if itâ€™s really stuck:
kubectl delete application payment-app -n argocd --force --grace-period=0

Final checklist (memorize this)
âœ” Root app lives outside bootstrap
âœ” Bootstrap contains only child Applications
âœ” Root app is created once
âœ” Root app never manages itself
âœ” Application names never change

# Minikube is not able to pull images from dockerhub thus you have to preload your images
Web App
1. docker pull dainmusty/phone-store:latest     # listens on port 80
2. minikube image load dainmusty/phone-store:latest

Token App
1. docker pull dainmusty/effulgencetech-nodejs-img:latest          # listens on port 8080
2. minikube image load dainmusty/effulgencetech-nodejs-img:tag

Payment App (web and database)
1. docker pull mongo:5.0          
2. minikube image load mongo:5.0

3. docker pull nanajanashia/k8s-demo-app:v1.0
4. minikube image load nanajanashia/k8s-demo-app:v1.0

# Kids webApp
1. docker pull dainmusty/kids-website:latest
2. docker pull seidut/zay:1.1
3. docker pull seidut/ayd:1.0
# Test the applications via GUI
# Application Access Options

# Option 1 â€” Port-forwarding (quick, not scalable)
kubectl port-forward svc/web-app -n web-app 8081:80

Pros
Fast to test
No extra resources

Cons
Manual per app
Port collisions
Not production-like
ğŸ‘‰ Used only for short-lived debugging.

# Option 2. â€” Ingress-based routing (recommended)

This option mirrors how applications are exposed in production.

# Step 1 â€” Normalize container ports

All applications listen on the same container port (8080).

containers:
  - name: web-app
    image: dainmusty/effulgencetech-nodejs-img:tag
    ports:
      - containerPort: 8080

Repeat for:

web-app

token-app

payment-app

# Step 2 â€” Normalize Services

Each Service exposes port 80, forwarding to 8080 inside the Pod.

ports:
  - port: 80
    targetPort: 8080

This ensures:

Ingress always talks to port 80

Containers remain consistent

What â€œnormalize all appsâ€ means (in your setup)

Normalization = making all your applications look the same from Kubernetesâ€™ point of view.

Specifically:

Every app listens on the same container port (e.g. 8080)
Kubernetes Services & Ingress donâ€™t need to care about app-specific ports anymore.

# Why the normalisation matters
Clean Ingress rules
Reusable Helm charts
Predictable ArgoCD health checks
Fewer configuration bugs

# Local DNS Setup (Windows)
1. Open Notepad as administrator and edit your hosts file

Host file location - C:\Windows\System32\drivers\etc

Change file type from Text Documents (*.txt) â†’ All Files to see host file

2. Open the host file and add the line below the very bottom
  127.0.0.1   apps.local 

âš ï¸ Make sure:

There is at least one space or tab

No # in front

No .txt extension

3. Save (Ctrl + S)

4. Flush DNS cache (important)

Open Command Prompt as Administrator and run:

ipconfig /flushdns


You should see:

Successfully flushed the DNS Resolver Cache.

âœ… Verify (this MUST work)
ping apps.local


Expected:

Pinging apps.local [127.0.0.1] with 32 bytes of data
Windows ignores hosts changes unless:

File saved with admin rights

DNS cache flushed

Git Bash / WSL does not edit Windows DNS

âœ… After ping works

Then your Minikube ingress URLs will work:
http://web.apps.local
http://token.apps.local
http://payment.apps.local
Next logical steps (your setup is ready)

Once DNS works, we can:

Add path-based Ingress YAML

Add health checks in ArgoCD

Prepare same layout for EKS + ALB

Do not move forward until ping apps.local works

Normalized standard (this is gold):
This confirms:
âœ” Pods are running
âœ” App listens on 8080
âœ” Service correctly forwards 80 â†’ 8080
Browser
 â†’ http://token.apps.local
 â†’ Ingress (80)
 â†’ Service token-app (80)
 â†’ Pod token-app (8080)
 â†’ Node app responds âœ…
Final â€œNormalize all appsâ€ rule (the right way)

Normalization means:

All apps are accessed via port 80 externally,
but internally they can listen on any port they want.

External (Ingress)
web.apps.local    â†’ 80
token.apps.local  â†’ 80
payment.apps.localâ†’ 80

Internal (Service â†’ Pod)
nginx    â†’ 80
node     â†’ 8080
python   â†’ 5000

Ingress never changes â€” Services adapt.
| App         | Container Port | Service Target | Ingress |
| ----------- | -------------- | -------------- | ------- |
| web-app     | 80             | 80             | 80      |
| token-app   | 8080           | 8080           | 80      |
| payment-app | 3000           | 3000           | 80      |


# To log into a pod and check app status and listening port
kubectl exec -it -n token-app <pod> -- sh
wget -O- http://localhost:8080


# Add HTTPS (TLS) locally with Minikube + nginx ingress, the production-correct way, without hacks.

Youâ€™ll end up with:

https://web.apps.local
https://token.apps.local
https://payment.apps.local


âœ” Valid local TLS
âœ” Browser trusted cert
âœ” Same pattern as EKS / ALB
âœ” No port-forwards needed

1ï¸âƒ£ Install & setup mkcert (local CA)

mkcert creates a trusted local Certificate Authority.

Windows (PowerShell â€“ Admin)
choco install mkcert


OR download from:
https://github.com/FiloSottile/mkcert/releases

Initialize local CA
mkcert -install


This adds a trusted root cert to your OS & browser.

2ï¸âƒ£ Generate certs for your apps

From any directory (repo root is fine):

mkcert \
  web.apps.local \
  token.apps.local \
  payment.apps.local


This creates two files:

web.apps.local+2.pem
web.apps.local+2-key.pem


(rename is fine, content matters)

3ï¸âƒ£ Create a Kubernetes TLS Secret (once)

Weâ€™ll create one shared TLS secret.

kubectl create secret tls apps-local-tls \
  --cert=web.apps.local+2.pem \
  --key=web.apps.local+2-key.pem \
  -n ingress-nginx


âœ… Stored where ingress controller runs
âœ… Reusable by all ingresses

Verify:

kubectl get secret -n ingress-nginx

4ï¸âƒ£ Update Ingress to use HTTPS
Example: token-app ingress (final)
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: token-app
  namespace: token-app
spec:
  ingressClassName: nginx

  tls:
    - hosts:
        - token.apps.local
      secretName: apps-local-tls

  rules:
    - host: token.apps.local
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: token-app
                port:
                  number: 80


Do the same TLS block for:

web-app

payment-app

Only the host + service name change.

5ï¸âƒ£ Restart ingress (important)
kubectl rollout restart deployment ingress-nginx-controller -n ingress-nginx

6ï¸âƒ£ Test ğŸ‰

Open browser:

https://token.apps.local
https://web.apps.local


âœ… Green lock
âœ… No browser warnings
âœ… Real TLS

ğŸ§  Key production lessons you just implemented
Local CA	mkcert
TLS secrets	Kubernetes-native
Ingress TLS	spec.tls
No hacks	No port-forward
Cloud parity	Matches ALB

This exact pattern maps directly to:

AWS ALB + ACM

GKE Managed Certs

AKS App Gateway

Only the cert source changes.

# NetworkPolicies are where Kubernetes becomes â€œreal securityâ€ ğŸ”
Default Kubernetes behavior (bad):

Every pod can talk to every pod in every namespace

Zero-trust model (good):

Nothing talks to anything unless explicitly allowed

Default deny per namespace

Allow ingress traffic only from ingress-nginx

Allow DNS (required!)

(Optional) Allow app-to-app communication later

1ï¸âƒ£ Prerequisite check (important)

NetworkPolicies only work if your CNI supports them.

Minikube drivers that support NetworkPolicy:

âœ… docker

âœ… containerd

âŒ none (old VM drivers)

Verify your CNI:

kubectl get pods -n kube-system | grep -E "calico|cilium|weave"


Minikube usually uses calico, so youâ€™re good.

2ï¸âƒ£ Default-deny policy (per app namespace)

This is the foundation of zero-trust.

ğŸ“„ networkpolicy-default-deny.yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-all
  namespace: token-app
spec:
  podSelector: {}
  policyTypes:
    - Ingress
    - Egress

What this does

âŒ Blocks ALL inbound traffic

âŒ Blocks ALL outbound traffic

Applies to every pod in token-app

âš ï¸ If you stop here â†’ app breaks (expected)

3ï¸âƒ£ Allow ingress traffic from nginx ingress controller

Your apps must accept traffic only from ingress-nginx.

ğŸ“„ networkpolicy-allow-ingress-nginx.yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-ingress-from-nginx
  namespace: token-app
spec:
  podSelector:
    matchLabels:
      app: token-app

  policyTypes:
    - Ingress

  ingress:
    - from:
        - namespaceSelector:
            matchLabels:
              kubernetes.io/metadata.name: ingress-nginx
      ports:
        - protocol: TCP
          port: 8080

Why this is secure

âœ” Only ingress controller can reach the app
âœ” No other namespace can talk to it
âœ” No pod-to-pod lateral movement

4ï¸âƒ£ Allow DNS (mandatory or everything breaks)

Every pod needs DNS to function.

ğŸ“„ networkpolicy-allow-dns.yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-dns
  namespace: token-app
spec:
  podSelector: {}
  policyTypes:
    - Egress
  egress:
    - to:
        - namespaceSelector:
            matchLabels:
              kubernetes.io/metadata.name: kube-system
      ports:
        - protocol: UDP
          port: 53
        - protocol: TCP
          port: 53


Without this:

âŒ Images wonâ€™t pull

âŒ Services wonâ€™t resolve

âŒ HTTP clients fail

5ï¸âƒ£ Apply same pattern to ALL apps

Repeat these 3 policies for:

web-app

payment-app

Only change:

namespace

app: label

target port (80 vs 8080)

ğŸ” Test 2 â€” block lateral traffic
kubectl exec -it -n web-app <pod> -- wget -O- http://token-app.token-app.svc.cluster.local


âŒ Should FAIL
âœ” Zero-trust confirmed

ğŸ” Test 3 â€” ArgoCD health

ArgoCD should remain Healthy because:

It reads Kubernetes state

It does NOT need network access to pods

7ï¸âƒ£ Folder structure (recommended)

Add this to each app:

apps/dev/token-app/
â”œâ”€â”€ deployment.yaml
â”œâ”€â”€ service.yaml
â”œâ”€â”€ ingress.yaml
â””â”€â”€ networkpolicies/
    â”œâ”€â”€ default-deny.yaml
    â”œâ”€â”€ allow-ingress-nginx.yaml
    â””â”€â”€ allow-dns.yaml


ArgoCD will apply them automatically.

ğŸ” What security you just implemented (real-world)

âœ” Namespace isolation
âœ” Ingress-only exposure
âœ” No lateral movement
âœ” DNS-only egress
âœ” Cloud-grade zero-trust

This is exactly what security teams demand in production EKS clusters.

# Installation of Prometheus and Grafana
# Use helm, its recommended 
1. helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
2. helm repo update
# Create a namespace monitoring
3. kubectl create namespace monitoring
4. helm install monitoring prometheus-community/kube-prometheus-stack --namespace monitoring
# Check status
5. kubectl get pod -n monitoring

# Grafana UI
kubectl port-forward -n monitoring svc/monitoring-grafana 3000:80
http://localhost:3000

# Grafana Password
kubectl get secret --namespace monitoring -l app.kubernetes.io/component=admin-secret -o jsonpath="{.items[0].data.admin-password}" | base64 --decode ; echo

kubectl get secret -n monitoring monitoring-grafana -o jsonpath="{.data.admin-password}" | base64 --decode

# Add prometheus as a datasource
url required - http://monitoring-kube-prometheus-prometheus.monitoring.svc:9090


# Prometheus
kubectl port-forward -n monitoring svc/monitoring-kube-prometheus-prometheus 9090:9090
http://localhost:9090

How to Verify the Service Name (Optional)
kubectl get svc -n monitoring



3. Verify Alertmanager (for TestAlert)
kubectl port-forward -n monitoring svc/monitoring-kube-prometheus-alertmanager 9093:9093
# Access locally via http://localhost:9093



# Persistence (PVC)

Ensure data survives pod restart
