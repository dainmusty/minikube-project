# Kubernetes-Argocd Project (App of Apps)
Project Overview
GitOps-Driven Microservices Deployment on Kubernetes (EKS / Minikube)
A production-style Kubernetes microservices project using GitOps with Argo CD.
Demonstrates secure service-to-service communication, ingress management, and MongoDB integration.
Focused on real-world debugging, Kustomize overlays, and operational best practices.

Steps
1. Deploy minikube or Docker Desktop Cluster
a. Run choco install minikube or 
b. Go to kubernetes on docker desktop and select kind cluster type

2.  Install argocd 
kubectl create namespace argocd
kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml
kubectl get pods -n argocd
kubectl port-forward svc/argocd-server -n argocd 8080:80
kubectl get secret argocd-initial-admin-secret -n argocd -o jsonpath="{.data.password}" | base64 --decode

3. Install nginx alb controller
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/cloud/deploy.yaml

4. Prepare your manifest files using the structure below
repo-root/
â”œâ”€â”€ k8s/
â”‚   â”œâ”€â”€ bootstrap/
â”‚   â”‚   â””â”€â”€ root-app-dev.yaml      # App of Apps 
â”‚   â””â”€â”€ apps/
â”‚       â””â”€â”€ dev/
â”‚           â”œâ”€â”€ web-app/
â”‚           â”‚   â”œâ”€â”€ deployment.yaml
â”‚           â”‚   â”œâ”€â”€ service.yaml
â”‚           â”‚   â”œâ”€â”€ ingress.yaml
â”‚           â”‚   
â”‚           â”œâ”€â”€ token-app/
â”‚           â”‚   â”œâ”€â”€ deployment.yaml
â”‚           â”‚   â”œâ”€â”€ service.yaml
â”‚           â”‚   â”œâ”€â”€ ingress.yaml
â”‚           â”‚   
â”‚           â””â”€â”€ payment-app/
â”‚               â”œâ”€â”€ mongo-deployment.yaml
â”‚               â”œâ”€â”€ mongo-service.yaml
â”‚               â”œâ”€â”€ web-deployment.yaml
â”‚               â”œâ”€â”€ web-service.yaml
â”‚               â”œâ”€â”€ configmap.yaml
â”‚               â”œâ”€â”€ secret.yaml
â”‚               â””â”€â”€ web-ingress.yaml

| Component         | Exposure       |
| ----------------- | -------------- |
| Web App           | Ingress        |
| Token App         | Ingress        |
| Payment (MongoDB) | ClusterIP only |
| Debugging         | Port-forward   |

# to delete applications that are stuck
Remove the finalizer (this is the key step)
Run:
kubectl patch application payment-app -n argocd --type=json -p='[{"op":"remove","path":"/metadata/finalizers"}]'
kubectl delete application payment-app -n argocd
kubectl rollout restart deployment argocd-server -n argocd
Nuclear option (rarely needed)
Only if itâ€™s really stuck:
kubectl delete application payment-app -n argocd --force --grace-period=0
But 90% of the time, removing the finalizer is enough.


# Minikube is not able to pull images from dockerhub thus you have to preload your images
Web App
1. docker pull dainmusty/phone-store:latest     # listens on port 80
2. minikube image load dainmusty/phone-store:latest

Token App
1. docker pull dainmusty/effulgencetech-nodejs-img:tag          # listens on port 8080
2. minikube image load dainmusty/effulgencetech-nodejs-img:tag

Payment App (web and database)
1. docker pull mongo:5.0          
2. minikube image load mongo:5.0

3. docker pull nanajanashia/k8s-demo-app:v1.0
4. minikube image load nanajanashia/k8s-demo-app:v1.0

# Test the applications via GUI

1. Option 1 - Port Forwarding
Port forward using its container port number (8081)
kubectl port-forward svc/web-app -n web-app 8081:80

2. Option 2 - Single Ingress, multiple paths (recommended)
Step 1: Normalize container ports (Deployment)
Each child app Deployment should expose 8080 only.

Example: web-app Deployment
containers:
  - name: web-app
    image: dainmusty/effulgencetech-nodejs-img:tag
    ports:
      - containerPort: 8080


Repeat for:

web-app

token-app

payment-app

âœ… No 3000, no 5000, no custom ports per app

Step 2: Normalize Services

Each Service exposes port 80, forwards to 8080.

Example: web-app Service
apiVersion: v1
kind: Service
metadata:
  name: web-app
  namespace: web-app
spec:
  type: ClusterIP
  selector:
    app: web-app
  ports:
    - port: 80
      targetPort: 8080


Repeat for:

token-app

payment-app

ğŸ‘‰ Internally, Kubernetes always talks to port 80, containers always listen on 8080

Step 3: Add Ingress (Minikube nginx)

Enable nginx if you havenâ€™t:

minikube addons enable ingress

What â€œnormalize all appsâ€ means (in your setup)

Normalization = making all your applications look the same from Kubernetesâ€™ point of view.

Specifically:

Every app listens on the same container port (e.g. 8080)
Kubernetes Services & Ingress donâ€™t need to care about app-specific ports anymore.

Why this matters

Ingress rules become clean and consistent

Helm / manifests are reusable

ArgoCD health checks behave predictably

You avoid port-mismatch bugs like the one you just debugged

1. Open Notepad as Administrator

This part is mandatory.

Click Start

Type Notepad

Right-click â†’ Run as administrator

2ï¸âƒ£ Open the hosts file

In Notepad:

File â†’ Open

Go to:

C:\Windows\System32\drivers\etc


Change file type from Text Documents (*.txt) â†’ All Files

Open hosts

3ï¸âƒ£ Add this line at the VERY BOTTOM
127.0.0.1   apps.local


âš ï¸ Make sure:

There is at least one space or tab

No # in front

No .txt extension

4ï¸âƒ£ Save (Ctrl + S)

If it doesnâ€™t ask for permission, you didnâ€™t open Notepad as admin â†’ repeat step 1.

5ï¸âƒ£ Flush DNS cache (important)

Open Command Prompt as Administrator and run:

ipconfig /flushdns


You should see:

Successfully flushed the DNS Resolver Cache.

âœ… Verify (this MUST work)
ping apps.local


Expected:

Pinging apps.local [127.0.0.1] with 32 bytes of data
Windows ignores hosts changes unless:

File saved with admin rights

DNS cache flushed

Git Bash / WSL does not edit Windows DNS

âœ… After ping works

Then your Minikube ingress URLs will work:
http://web.apps.local
http://token.apps.local
http://payment.apps.local
Next logical steps (your setup is ready)

Once DNS works, we can:

Add path-based Ingress YAML

Add health checks in ArgoCD

Prepare same layout for EKS + ALB

Do not move forward until ping apps.local works

Normalized standard (this is gold):
This confirms:
âœ” Pods are running
âœ” App listens on 8080
âœ” Service correctly forwards 80 â†’ 8080
Browser
 â†’ http://token.apps.local
 â†’ Ingress (80)
 â†’ Service token-app (80)
 â†’ Pod token-app (8080)
 â†’ Node app responds âœ…
Final â€œNormalize all appsâ€ rule (the right way)

Normalization means:

All apps are accessed via port 80 externally,
but internally they can listen on any port they want.

External (Ingress)
web.apps.local    â†’ 80
token.apps.local  â†’ 80
payment.apps.localâ†’ 80

Internal (Service â†’ Pod)
nginx    â†’ 80
node     â†’ 8080
python   â†’ 5000

Ingress never changes â€” Services adapt.
| App         | Container Port | Service Target | Ingress |
| ----------- | -------------- | -------------- | ------- |
| web-app     | 80             | 80             | 80      |
| token-app   | 8080           | 8080           | 80      |
| payment-app | 3000           | 3000           | 80      |


# To log into a pod and check app status and listening port
kubectl exec -it -n token-app <pod> -- sh
wget -O- http://localhost:8080


# Add HTTPS (TLS) locally with Minikube + nginx ingress, the production-correct way, without hacks.

Youâ€™ll end up with:

https://web.apps.local
https://token.apps.local
https://payment.apps.local


âœ” Valid local TLS
âœ” Browser trusted cert
âœ” Same pattern as EKS / ALB
âœ” No port-forwards needed

1ï¸âƒ£ Install & setup mkcert (local CA)

mkcert creates a trusted local Certificate Authority.

Windows (PowerShell â€“ Admin)
choco install mkcert


OR download from:
https://github.com/FiloSottile/mkcert/releases

Initialize local CA
mkcert -install


This adds a trusted root cert to your OS & browser.

2ï¸âƒ£ Generate certs for your apps

From any directory (repo root is fine):

mkcert \
  web.apps.local \
  token.apps.local \
  payment.apps.local


This creates two files:

web.apps.local+2.pem
web.apps.local+2-key.pem


(rename is fine, content matters)

3ï¸âƒ£ Create a Kubernetes TLS Secret (once)

Weâ€™ll create one shared TLS secret.

kubectl create secret tls apps-local-tls \
  --cert=web.apps.local+2.pem \
  --key=web.apps.local+2-key.pem \
  -n ingress-nginx


âœ… Stored where ingress controller runs
âœ… Reusable by all ingresses

Verify:

kubectl get secret -n ingress-nginx

4ï¸âƒ£ Update Ingress to use HTTPS
Example: token-app ingress (final)
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: token-app
  namespace: token-app
spec:
  ingressClassName: nginx

  tls:
    - hosts:
        - token.apps.local
      secretName: apps-local-tls

  rules:
    - host: token.apps.local
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: token-app
                port:
                  number: 80


Do the same TLS block for:

web-app

payment-app

Only the host + service name change.

5ï¸âƒ£ Restart ingress (important)
kubectl rollout restart deployment ingress-nginx-controller -n ingress-nginx

6ï¸âƒ£ Test ğŸ‰

Open browser:

https://token.apps.local
https://web.apps.local


âœ… Green lock
âœ… No browser warnings
âœ… Real TLS

ğŸ§  Key production lessons you just implemented
Local CA	mkcert
TLS secrets	Kubernetes-native
Ingress TLS	spec.tls
No hacks	No port-forward
Cloud parity	Matches ALB

This exact pattern maps directly to:

AWS ALB + ACM

GKE Managed Certs

AKS App Gateway

Only the cert source changes.

# NetworkPolicies are where Kubernetes becomes â€œreal securityâ€ ğŸ”
Default Kubernetes behavior (bad):

Every pod can talk to every pod in every namespace

Zero-trust model (good):

Nothing talks to anything unless explicitly allowed

Default deny per namespace

Allow ingress traffic only from ingress-nginx

Allow DNS (required!)

(Optional) Allow app-to-app communication later

1ï¸âƒ£ Prerequisite check (important)

NetworkPolicies only work if your CNI supports them.

Minikube drivers that support NetworkPolicy:

âœ… docker

âœ… containerd

âŒ none (old VM drivers)

Verify your CNI:

kubectl get pods -n kube-system | grep -E "calico|cilium|weave"


Minikube usually uses calico, so youâ€™re good.

2ï¸âƒ£ Default-deny policy (per app namespace)

This is the foundation of zero-trust.

ğŸ“„ networkpolicy-default-deny.yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-all
  namespace: token-app
spec:
  podSelector: {}
  policyTypes:
    - Ingress
    - Egress

What this does

âŒ Blocks ALL inbound traffic

âŒ Blocks ALL outbound traffic

Applies to every pod in token-app

âš ï¸ If you stop here â†’ app breaks (expected)

3ï¸âƒ£ Allow ingress traffic from nginx ingress controller

Your apps must accept traffic only from ingress-nginx.

ğŸ“„ networkpolicy-allow-ingress-nginx.yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-ingress-from-nginx
  namespace: token-app
spec:
  podSelector:
    matchLabels:
      app: token-app

  policyTypes:
    - Ingress

  ingress:
    - from:
        - namespaceSelector:
            matchLabels:
              kubernetes.io/metadata.name: ingress-nginx
      ports:
        - protocol: TCP
          port: 8080

Why this is secure

âœ” Only ingress controller can reach the app
âœ” No other namespace can talk to it
âœ” No pod-to-pod lateral movement

4ï¸âƒ£ Allow DNS (mandatory or everything breaks)

Every pod needs DNS to function.

ğŸ“„ networkpolicy-allow-dns.yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-dns
  namespace: token-app
spec:
  podSelector: {}
  policyTypes:
    - Egress
  egress:
    - to:
        - namespaceSelector:
            matchLabels:
              kubernetes.io/metadata.name: kube-system
      ports:
        - protocol: UDP
          port: 53
        - protocol: TCP
          port: 53


Without this:

âŒ Images wonâ€™t pull

âŒ Services wonâ€™t resolve

âŒ HTTP clients fail

5ï¸âƒ£ Apply same pattern to ALL apps

Repeat these 3 policies for:

web-app

payment-app

Only change:

namespace

app: label

target port (80 vs 8080)

Example port mapping
App	NetworkPolicy port
web-app	80
token-app	8080
payment-app	8080
6ï¸âƒ£ Verify zero-trust is working
ğŸ” Test 1 â€” ingress still works
https://token.apps.local

ğŸ” Test 2 â€” block lateral traffic
kubectl exec -it -n web-app <pod> -- wget -O- http://token-app.token-app.svc.cluster.local


âŒ Should FAIL
âœ” Zero-trust confirmed

ğŸ” Test 3 â€” ArgoCD health

ArgoCD should remain Healthy because:

It reads Kubernetes state

It does NOT need network access to pods

7ï¸âƒ£ Folder structure (recommended)

Add this to each app:

apps/dev/token-app/
â”œâ”€â”€ deployment.yaml
â”œâ”€â”€ service.yaml
â”œâ”€â”€ ingress.yaml
â””â”€â”€ networkpolicies/
    â”œâ”€â”€ default-deny.yaml
    â”œâ”€â”€ allow-ingress-nginx.yaml
    â””â”€â”€ allow-dns.yaml


ArgoCD will apply them automatically.

ğŸ” What security you just implemented (real-world)

âœ” Namespace isolation
âœ” Ingress-only exposure
âœ” No lateral movement
âœ” DNS-only egress
âœ” Cloud-grade zero-trust

This is exactly what security teams demand in production EKS clusters.

# Persistence (PVC)

Ensure data survives pod restart
